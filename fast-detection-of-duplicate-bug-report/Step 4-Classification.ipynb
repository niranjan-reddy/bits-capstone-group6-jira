{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ra4uAoXwKDVI"
   },
   "source": [
    "# **Classification using Unified Similarity Measure**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 83
    },
    "colab_type": "code",
    "id": "Uo_NlNDhSKXH",
    "outputId": "67e4030c-a6f1-42a4-c974-b86b687b6adc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import gensim\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "import numpy as np\n",
    "from gensim import corpora,models\n",
    "import time\n",
    "import pickle\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import warnings\n",
    "from gensim.models import Word2Vec,FastText\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy.spatial import distance\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 216
    },
    "colab_type": "code",
    "id": "4n62yJhof_Fv",
    "outputId": "fd5df092-0edd-436f-938f-c9066c9036bc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting glove_python\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3e/79/7e7e548dd9dcb741935d031117f4bed133276c2a047aadad42f1552d1771/glove_python-0.1.0.tar.gz (263kB)\n",
      "\r",
      "\u001b[K     |█▎                              | 10kB 15.3MB/s eta 0:00:01\r",
      "\u001b[K     |██▌                             | 20kB 3.2MB/s eta 0:00:01\r",
      "\u001b[K     |███▊                            | 30kB 3.5MB/s eta 0:00:01\r",
      "\u001b[K     |█████                           | 40kB 3.0MB/s eta 0:00:01\r",
      "\u001b[K     |██████▎                         | 51kB 3.2MB/s eta 0:00:01\r",
      "\u001b[K     |███████▌                        | 61kB 3.8MB/s eta 0:00:01\r",
      "\u001b[K     |████████▊                       | 71kB 4.0MB/s eta 0:00:01\r",
      "\u001b[K     |██████████                      | 81kB 3.7MB/s eta 0:00:01\r",
      "\u001b[K     |███████████▏                    | 92kB 4.2MB/s eta 0:00:01\r",
      "\u001b[K     |████████████▌                   | 102kB 4.4MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████▊                  | 112kB 4.4MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████                 | 122kB 4.4MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████▏               | 133kB 4.4MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████▍              | 143kB 4.4MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████▊             | 153kB 4.4MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████            | 163kB 4.4MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████▏          | 174kB 4.4MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████▍         | 184kB 4.4MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████▋        | 194kB 4.4MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████       | 204kB 4.4MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████▏     | 215kB 4.4MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████▍    | 225kB 4.4MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████▋   | 235kB 4.4MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████████▉  | 245kB 4.4MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████████▏| 256kB 4.4MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 266kB 4.4MB/s \n",
      "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from glove_python) (1.18.3)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from glove_python) (1.4.1)\n",
      "Building wheels for collected packages: glove-python\n",
      "  Building wheel for glove-python (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for glove-python: filename=glove_python-0.1.0-cp36-cp36m-linux_x86_64.whl size=700266 sha256=ccaecdbda5f64d332897bfcebcd445611370c39b928475d1a504bdec98c50d16\n",
      "  Stored in directory: /root/.cache/pip/wheels/88/4b/6d/10c0d2ad32c9d9d68beec9694a6f0b6e83ab1662a90a089a4b\n",
      "Successfully built glove-python\n",
      "Installing collected packages: glove-python\n",
      "Successfully installed glove-python-0.1.0\n"
     ]
    }
   ],
   "source": [
    "# !pip install glove_python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OE2FYbCkgGxJ"
   },
   "outputs": [],
   "source": [
    "from glove import Glove, Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "colab_type": "code",
    "id": "UGn76yTmSRLd",
    "outputId": "49ea23d5-8930-44b8-c344-e170a417e113"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:253: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
      "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
     ]
    }
   ],
   "source": [
    "# open a file, where you stored the pickled data\n",
    "f = open('dataset/bow_corpus.pickle', 'rb')\n",
    "bow_corpus=pickle.load(f)\n",
    "\n",
    "file = open('dataset/dictionary.pickle', 'rb')\n",
    "dictionary=pickle.load(file)\n",
    "\n",
    "# later on, load trained model from file\n",
    "lda_model =  models.LdaModel.load('dataset/lda_model.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mqJMzJoFSqX3"
   },
   "outputs": [],
   "source": [
    "def lemmatize(text):\n",
    "    return WordNetLemmatizer().lemmatize(text, pos='v')\n",
    "\n",
    "def preprocess(text):\n",
    "    result = []\n",
    "    for token in gensim.utils.simple_preprocess(text):\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 5:\n",
    "            result.append(lemmatize(token))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WEt1TXMYScSZ"
   },
   "outputs": [],
   "source": [
    "\n",
    "# importing all the clusters created using LDA based topic modeling\n",
    "for c in range(10):\n",
    "  exec('topic_{} = pd.read_csv(\"dataset/topic_{}.csv\")'.format(c,c))\n",
    "  exec(\"topic_{}= topic_{}.drop(columns=['Unnamed: 0'])\".format(c,c))\n",
    "  exec(\"topic_{}['Description'] = topic_{}['Description'].map(preprocess)\".format(c,c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "p8ar7N6mfBSN"
   },
   "outputs": [],
   "source": [
    "#import the duplicate reports for testing purpose\n",
    "test = pd.read_csv('dataset/duplicate_reports.csv')\n",
    "test = test.drop(columns=['Unnamed: 0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "V1NfsKaphgGs"
   },
   "outputs": [],
   "source": [
    "test['Description']= test['Description'].map(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "colab_type": "code",
    "id": "3IsHybD6SlgC",
    "outputId": "25e968b6-c076-4ee0-c104-48f218bb1310"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:253: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
      "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
     ]
    }
   ],
   "source": [
    "for mod in range(10):\n",
    "  #import all the trained Word2Vec models\n",
    "  exec('w2vmodel{} = Word2Vec.load(\"dataset/word2vec{}.model\")'.format(mod, mod))\n",
    "\n",
    "  #import all the trained FastText models\n",
    "  exec('ftmodel{} = FastText.load(\"dataset/fastText{}.model\")'.format(mod, mod))\n",
    "\n",
    "  #import all the trained GloVe models\n",
    "  exec('glove{} = Glove.load(\"dataset/glove{}.model\")'.format(mod, mod))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZleuynPDU2GI"
   },
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wYKIDs4VKhFg"
   },
   "source": [
    "### **Selection of Top-n clusters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "S6FFJvIdVVBZ"
   },
   "outputs": [],
   "source": [
    "#This will return the index of cluster in which the master report of duplicate report may reside\n",
    "def sim_with_clusters_lda_topn(DR, n):\n",
    "    vec_bow = dictionary.doc2bow(DR)\n",
    "    x= lda_model[vec_bow]\n",
    "    topic = np.asarray(x)\n",
    "    # max_sim = int(topic[np.argmax(topic[:,1]),0]) \n",
    "    # max_sim\n",
    "    sim=[]\n",
    "    x= topic[np.argsort(topic[:,1])[-n:][::-1],0]\n",
    "    for i in range(len(x)):\n",
    "        sim.append(int(x[i]))\n",
    "    # return max_sim\n",
    "    return sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LsPY5Sicg0ZC"
   },
   "outputs": [],
   "source": [
    "# Convert multiple word embeddings into a single document vector by averaging the word embeddings by GloVe model\n",
    "\n",
    "def average_word_vectors_glove(words, model, vocabulary, num_features):  \n",
    "    feature_vector = np.zeros((num_features,),dtype=\"float64\")\n",
    "    nwords = 0.  \n",
    "\n",
    "    for word in words:\n",
    "        if word in vocabulary: \n",
    "            nwords = nwords + 1.\n",
    "            feature_vector = np.add(feature_vector, model.word_vectors[model.dictionary[word]])\n",
    "\n",
    "    if nwords:\n",
    "        feature_vector = np.divide(feature_vector, nwords)\n",
    "        \n",
    "    return feature_vector\n",
    "    \n",
    "\n",
    "\n",
    "def averaged_word_vectorizer_glove(corpus, model, num_features):\n",
    "    vocabulary = set(model.dictionary)\n",
    "    if(any(isinstance(i, list) for i in corpus)):\n",
    "      features = [average_word_vectors_glove(tokenized_sentence, model, vocabulary, num_features)\n",
    "                      for tokenized_sentence in corpus]\n",
    "      return np.array(features)\n",
    "    else:\n",
    "      features = average_word_vectors_glove(corpus, model, vocabulary, num_features)\n",
    "      return np.array(features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BGo2DWb-TvvZ"
   },
   "outputs": [],
   "source": [
    "# Convert multiple word embeddings into a single document vector by averaging the word embeddings by FastText or Word2Vec model\n",
    "\n",
    "def average_word_vectors_w2v(words, model, vocabulary, num_features):\n",
    "    \n",
    "    feature_vector = np.zeros((num_features,),dtype=\"float64\")\n",
    "    nwords = 0.\n",
    "    \n",
    "    for word in words:\n",
    "        if word in vocabulary: \n",
    "            nwords = nwords + 1.\n",
    "            feature_vector = np.add(feature_vector, model[word])\n",
    "    \n",
    "    if nwords:\n",
    "        feature_vector = np.divide(feature_vector, nwords)\n",
    "        \n",
    "    return feature_vector\n",
    "    \n",
    "def averaged_word_vectorizer_w2v(corpus, model, num_features):\n",
    "    vocabulary = set(model.wv.index2word)\n",
    "    if(any(isinstance(i, list) for i in corpus)):\n",
    "      features = [average_word_vectors_w2v(tokenized_sentence, model, vocabulary, num_features)\n",
    "                      for tokenized_sentence in corpus]\n",
    "      return np.array(features)\n",
    "    else:\n",
    "      features = average_word_vectors_w2v(corpus, model, vocabulary, num_features)\n",
    "      return np.array(features)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cievu27fKtCH"
   },
   "source": [
    "### **Unified Similarity Measure**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mSrDdldatps-"
   },
   "outputs": [],
   "source": [
    "# Similarity between two feature vectors using the average of cosine similarity and euclidean similarity\n",
    "def sim(vec1, vec2): \n",
    "  sim1 = 1/(1+distance.euclidean(vec1, vec2))\n",
    "  sim2 = cosine_similarity(vec1, vec2)\n",
    "  sim=(sim1+sim2)/2 \n",
    "  return sim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FUB6hIlrKykY"
   },
   "source": [
    "### **Multimodality Fusions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "z4A0FX-ry0B0"
   },
   "outputs": [],
   "source": [
    "# Different kinds of fusion of two master report feature vectors and two duplicate report feature vectors\n",
    "def fusion(vec1, vec2, vec3, vec4, fusion_no):\n",
    "\n",
    "  # fusion_no = 1 : concatenation of the vectors\n",
    "  if (fusion_no == '1'):\n",
    "    master = np.concatenate((vec1, vec2), axis=1)\n",
    "    vec_duplicate = np.concatenate((vec3, vec4), axis=0)\n",
    "    vec_duplicate=[vec_duplicate]\n",
    "    return vec_duplicate, master\n",
    "\n",
    "  #fusion_no = 2 : average of the vectors\n",
    "  elif (fusion_no == '2'):\n",
    "    vec3 = [vec3]\n",
    "    vec4 = [vec4]\n",
    "    avg1 = (np.add(vec1, vec2))/2\n",
    "    avg2 = (np.add(vec3, vec4))/2\n",
    "    return avg2, avg1\n",
    "\n",
    "  #fusion_no = 3 : Dimensionality reduction using PCA on concatenation of the vectors\n",
    "  elif (fusion_no == '3'):\n",
    "    master = np.concatenate((vec1, vec2), axis=1)\n",
    "    pca = PCA(n_components=100)\n",
    "    avg_fit = pca.fit(master)\n",
    "    master = pca.transform(master)\n",
    "    vec_duplicate = np.concatenate((vec3, vec4), axis=0)\n",
    "    vec_duplicate=[vec_duplicate]\n",
    "    vec_duplicate = pca.transform(vec_duplicate)\n",
    "    return vec_duplicate, master\n",
    "\n",
    "  #fusion_no = 3 : Dimensionality reduction using PCA on average of the vectors\n",
    "  elif (fusion_no == '4'):\n",
    "    vec3 = [vec3]\n",
    "    vec4 = [vec4]\n",
    "    avg1 = (np.add(vec1, vec2))/2\n",
    "    pca = PCA(n_components=100)\n",
    "    avg_fit = pca.fit(avg1)\n",
    "    master = pca.transform(avg1)\n",
    "    avg2 = (np.add(vec3, vec4))/2\n",
    "    vec_duplicate = pca.transform(avg2)\n",
    "    return vec_duplicate, master\n",
    "\n",
    "  else:\n",
    "    raise ValueError('Invalid value for fusion')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SQ1TY4TrK9_0"
   },
   "source": [
    "### **Creation of Feature Vectors using Multimodality and Single modality Feature Extraction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Nii-n-0GsaKi"
   },
   "outputs": [],
   "source": [
    "# creation of feature vectors by multimodality feature extraction\n",
    "def feature_vectors_multi_modality(DR, corpus, model1, model2, fusion_no):\n",
    "  master_ft1 = averaged_word_vectorizer_w2v(corpus=sent, model=model1, num_features=100)\n",
    "  master_glove2 = averaged_word_vectorizer_glove(corpus=sent, model=model2, num_features=100)\n",
    "\n",
    "  vec_duplicate1 = averaged_word_vectorizer_w2v(corpus=DR, model=model1, num_features=100)\n",
    "  vec_duplicate2 = averaged_word_vectorizer_glove(corpus=DR, model=model2, num_features=100)\n",
    "\n",
    "  #for fusion 1 and fusion 3 :\n",
    "  # vec_duplicate, master= fusion_3(master_ft1, master_glove2, vec_duplicate1, vec_duplicate2)\n",
    "\n",
    "  #for this for fusion 2 and 4:\n",
    "  vec_duplicate , master= fusion(master_ft1, master_glove2, vec_duplicate1, vec_duplicate2, fusion_no)\n",
    "\n",
    "  return vec_duplicate,master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "u42AbKQ6GOiW"
   },
   "outputs": [],
   "source": [
    "# creation of feature vectors by singlemodality feature extraction\n",
    "def feature_vectors_single_modality(DR, corpus, model1):\n",
    "  master = averaged_word_vectorizer_w2v(corpus=sent, model=model1, num_features=100)\n",
    "\n",
    "  vec_duplicate = averaged_word_vectorizer_w2v(corpus=DR, model=model1, num_features=100)\n",
    "\n",
    "  vec_duplicate = [vec_duplicate]\n",
    "\n",
    "  return vec_duplicate, master"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nIFp0iS1LIou"
   },
   "source": [
    "### **Top-N Recommendations**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SMbKNltgU7H8"
   },
   "outputs": [],
   "source": [
    "# Returns Top-N master reports\n",
    "\n",
    "def compare_topn(model1, model2, cluster, sent, DR, topn, modal, fusion_no):\n",
    "  similarity=[]\n",
    "\n",
    "  if (modal == 'multi'):\n",
    "  #create feature vectors for duplicate and master reports using multimodality\n",
    "    vec_duplicate, master= feature_vectors_multi_modality(DR, sent, model1, model2, fusion_no)\n",
    "\n",
    "  # #create feature vectors for duplicate and master reports using single modality\n",
    "  elif (modal == 'single'):\n",
    "    vec_duplicate, master= feature_vectors_single_modality(DR, sent, model1)\n",
    "\n",
    "  else:\n",
    "    raise ValueError('Invalid Modality entered')\n",
    "\n",
    "  for doc in range(len(master)):\n",
    "    vec_master = master[doc]\n",
    "    vec_master= [vec_master]\n",
    "    unified_sim = sim(vec_duplicate, vec_master)\n",
    "\n",
    "    similarity.append(unified_sim)\n",
    "  similarity = np.asarray(similarity)\n",
    "  similarity= np.concatenate(similarity, axis=0 )\n",
    "  similarity= np.concatenate(similarity, axis=0 )\n",
    "  max_similar_reports=similarity.argsort()[-topn:][::-1]\n",
    "  # # # for d,f in enumerate(max_similar_reports):\n",
    "  # # #     similar_reports= similar_reports.append(cluster.loc[[f]])\n",
    "  return(max_similar_reports)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2iW_dNSPLOPz"
   },
   "source": [
    "### **Evalation of the Approach using Recall Rate**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HFWTB2VlVF_E"
   },
   "outputs": [],
   "source": [
    "# Recall Rate for Top-2.5K reports (Because Top-N where N = n * topn (2.5K = 3*833)) \n",
    "vec_acc=[]\n",
    "t1 = time.time()\n",
    "no_of_test_samples= int(200)\n",
    "for i in range(no_of_test_samples):\n",
    "  sample = test.Description[i] #The test sample (duplicate report)\n",
    "  n = 3\n",
    "  max_cluster =sim_with_clusters_lda_topn(sample, n)\n",
    "  v=[]\n",
    "  print(i)\n",
    "  for max in max_cluster:\n",
    "    exec('cluster = topic_{}'.format(max))              #The predicted cluster\n",
    "    exec('model1 = ftmodel{}'.format(max))              #The trained FastText model for the predicted cluster   (can be changed to other model as well viz. glove or word2vec)\n",
    "    exec('model2 = glove{}'.format(max))                #The trained Word2Vec model for the predicted cluster   (Doesn't count if using single modality)\n",
    "    exec('sent = topic_{}.Description'.format(max))     #The vocabulary for the predicted cluster\n",
    "    topn = 833                                          #The number of predicted master report for single predicted cluster\n",
    "    fusion_no = '4'   #Doesn't count if single modality #The selection of fusion used to fuse the word embeddings of two different models  (4 gives the best results)\n",
    "    modal = 'multi'                                    #Whether you want to use single feature extraction model or multi model ( for single, it'll consider just model1)\n",
    "     #This will return the Top-N predicted master reports\n",
    "    max_sim = compare_topn(model1, model2, cluster, sent, sample, topn, modal, fusion_no)\n",
    "    t2 = time.time()\n",
    "\n",
    "    #Comparing the predicted value to the ground truth\n",
    "    for num in max_sim:\n",
    "      if (cluster.Issue_id[num] == test.Duplicated_issue[i]):\n",
    "          v.append(\"1\")\n",
    "      else:\n",
    "          v.append(\"0\")\n",
    "  \n",
    "  if(all(x==v[0] for x in v)):\n",
    "    vec_acc.append(\"0\")\n",
    "  else:\n",
    "    vec_acc.append(\"1\")\n",
    "\n",
    "#Evaluating the performance by Recall Rate\n",
    "sum = 0\n",
    "for i,num in enumerate(vec_acc):\n",
    "    sum = sum + int(num)\n",
    "recall_rate = (sum/len(vec_acc))*100\n",
    "print(\"Recall Rate : {} %\".format(recall_rate))\n",
    "print(\"Time : \", (t2-t1)/60, \"min\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "a0WnRnwfIbOY"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Classification.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
