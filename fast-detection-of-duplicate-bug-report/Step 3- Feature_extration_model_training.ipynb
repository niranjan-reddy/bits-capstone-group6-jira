{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Cj0uxaDyXPoY"
   },
   "source": [
    "# **Feature Extraction Models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 83
    },
    "colab_type": "code",
    "id": "xm0lbbfSFw_n",
    "outputId": "d5b1c6ff-83e8-4edf-ba26-c080b5327ba2"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\niranjans3ln\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\niranjans3ln\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import gensim\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "import numpy as np\n",
    "from gensim import corpora,models\n",
    "import time\n",
    "import pickle\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "colab_type": "code",
    "id": "hyhp-dKUHEed",
    "outputId": "6ac21a34-1db1-4aef-96ed-95c0c24299d3"
   },
   "outputs": [
    {
     "ename": "EOFError",
     "evalue": "Ran out of input",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mEOFError\u001b[0m                                  Traceback (most recent call last)",
      "Input \u001b[1;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# open a file, where you stored the pickled data\u001b[39;00m\n\u001b[0;32m      2\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdataset/bow_corpus.pickle\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m bow_corpus\u001b[38;5;241m=\u001b[39m\u001b[43mpickle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m file \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdataset/dictionary.pickle\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      6\u001b[0m dictionary\u001b[38;5;241m=\u001b[39mpickle\u001b[38;5;241m.\u001b[39mload(file)\n",
      "\u001b[1;31mEOFError\u001b[0m: Ran out of input"
     ]
    }
   ],
   "source": [
    "# open a file, where you stored the pickled data\n",
    "f = open('dataset/bow_corpus.pickle', 'rb')\n",
    "bow_corpus=pickle.load(f)\n",
    "\n",
    "file = open('dataset/dictionary.pickle', 'rb')\n",
    "dictionary=pickle.load(file)\n",
    "\n",
    "# later on, load trained model from file\n",
    "lda_model =  models.LdaModel.load('dataset/lda_model.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9Dm8jFprLADw"
   },
   "outputs": [],
   "source": [
    "def lemmatize(text):\n",
    "    return WordNetLemmatizer().lemmatize(text, pos='v')\n",
    "\n",
    "def preprocess(text):\n",
    "    result = []\n",
    "    for token in gensim.utils.simple_preprocess(text):\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 5:\n",
    "            result.append(lemmatize(token))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wpCTWfbzK8sa"
   },
   "outputs": [],
   "source": [
    "#Import all the clusters from the drive \n",
    "for c in range(10):\n",
    "  exec('topic_{} = pd.read_csv(\"dataset/Mozilla/topic_{}.csv\")'.format(c,c))\n",
    "  exec(\"topic_{}= topic_{}.drop(columns=['Unnamed: 0'])\".format(c,c))\n",
    "  exec(\"topic_{}['Description'] = topic_{}['Description'].map(preprocess)\".format(c,c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oiZ93ze1VI6_"
   },
   "outputs": [],
   "source": [
    "#creating a corpus for Word2Vec and FastText models\n",
    "for i in range(10):\n",
    "  exec('sent_{} = []'.format(i))\n",
    "  exec('x= topic_{}'.format(i))\n",
    "  for j in range(len(x)):\n",
    "    exec('sent_{}.append(topic_{}.Description[{}])'.format(i,i,j))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 183
    },
    "colab_type": "code",
    "id": "yyBtELSBhZkw",
    "outputId": "6a439bb7-80fb-487a-dd17-3be924f4f6b0"
   },
   "outputs": [],
   "source": [
    "#Length of all the corpus\n",
    "for sent in range(10):\n",
    "  exec('print(len(sent_{}))'.format(sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7MN3B8FMcFRJ"
   },
   "outputs": [],
   "source": [
    "# !pip install glove_python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aaAVwfqHVM3I"
   },
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec,FastText\n",
    "from gensim.test.utils import get_tmpfile\n",
    "from glove import Glove, Corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "he0V2e8PYFpy"
   },
   "source": [
    "### **Word2Vec model training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "n82GYprkVU_U"
   },
   "outputs": [],
   "source": [
    "for cluster in range(10):\n",
    "  #Preparing parameters for model training\n",
    "  exec('corpus = sent_{}'.format(cluster))\n",
    "  vector_size = 100\n",
    "  w = 6\n",
    "  min_count = 5\n",
    "  CBoW = 0\n",
    "  iterations = 100\n",
    "\n",
    "  #Training Word2Vec model for each cluster\n",
    "  exec ('w2vmodel{} = Word2Vec(corpus, size=vector_size, window=w, min_count=min_count, sg=CBoW, iter=iterations)'.format(cluster))\n",
    "\n",
    "  #Save the all the models in individual file\n",
    "  exec('path = get_tmpfile(\"dataset/word2vec{}.model\")'.format(cluster))\n",
    "  exec('w2vmodel{}.save(\"dataset/word2vec{}.model\")'.format(cluster), cluster))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dqxoHhzCZIVi"
   },
   "source": [
    "### **FastText model training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "colab_type": "code",
    "id": "_g6HDHEylpRh",
    "outputId": "33629426-000f-476f-e4b8-4b60dc34a747"
   },
   "outputs": [],
   "source": [
    "for cluster in range(10):\n",
    "  #Preparing parameters for model training\n",
    "  exec('corpus = sent_{}'.format(cluster))\n",
    "  vector_size = 100\n",
    "  w = 6\n",
    "  min_count = 5\n",
    "  CBoW = 0\n",
    "  iterations = 100\n",
    "\n",
    "  #Training FastText model for each cluster\n",
    "  exec ('ftmodel{} = FastText(corpus, size=vector_size, window=w, min_count=min_count, sg=CBoW, iter=iterations)'.format(cluster))\n",
    "\n",
    "  #Save the all the models in individual file\n",
    "  exec('path = get_tmpfile(\"dataset/ftmodel{}.model\")'.format(cluster))\n",
    "  exec('ftmodel{}.save(\"dataset/ftmodel{}.model\")'.format(cluster, cluster))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kCdnHdsUc3Dm"
   },
   "source": [
    "### **GloVe model training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pjqrrgitqHgX"
   },
   "outputs": [],
   "source": [
    "#Training GloVe model for each cluster\n",
    "for cluster in range(10):\n",
    "  vector_size = 100\n",
    "  exec('glove_corpus{}=Corpus()'.format(cluster, cluster)) \n",
    "  exec('glove_corpus{}.fit(sent_{})'.format(cluster, cluster))\n",
    "  exec('glove{}= Glove(no_components=vector_size, learning_rate=0.18, alpha=0.75, max_count=100, max_loss=10.0, random_state=None)'.format(cluster, cluster))\n",
    "  exec('glove{}.fit(glove_corpus{}.matrix, epochs=1000, no_threads=3, verbose=True)'.format(cluster, cluster))\n",
    "  exec('transformer = lambda dictionary2:glove{}.transform_paragraph(words, epochs=1000,ignore_missing=False)'.format(cluster, cluster))\n",
    "  exec('glove{}.add_dictionary(glove_corpus{}.dictionary)'.format(cluster, cluster))\n",
    "\n",
    "  #Save the all the models in individual file\n",
    "  exec('path = get_tmpfile(\"dataset/glove{}.model\")'.format(cluster))\n",
    "  exec('glove{}.save(\"dataset/glove{}.model\")'.format(cluster, cluster))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "feature_extration_model_training.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
